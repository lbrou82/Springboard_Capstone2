{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split into training and testing data \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator # for image processing\n",
    "from keras.models import Sequential,load_model #used to build initial model \n",
    "from keras.layers import Activation, Dense, Dropout, Flatten #Dense used to add layers to model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping \n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD #this is for optimization on learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will run our project from end to end with this number of images to test run time \n",
    "\n",
    "IMG_COUNT = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the computing power on my personal laptop is limited, I will run the project end to end with a smaller amount of the image data, and will gradually increase the numbeer of images over time. \n",
    "\n",
    "To this end, we'll import our original file, and grab a random sample of images to train, validate, and test on. Further, to balance out our dataset, we'll take an equal number of images showing birds and an equal number that are not birds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clean annotation file\n",
    "df = pd.read_csv('../data/interim/wildlife_interim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random sample of images -- half birds, and half not birds\n",
    "df_bird = df[df.is_bird == 1].sample(n=int(IMG_COUNT/2), random_state=42)\n",
    "df_no_bird = df[df.is_bird == 0].sample(n=int(IMG_COUNT/2),random_state=42)\n",
    "df = pd.concat([df_bird, df_no_bird], axis=0)\n",
    "df.is_bird = df.is_bird.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Images:  2240\n",
      "Number of Validation Images:  800\n",
      "Number of Test Images:  960\n"
     ]
    }
   ],
   "source": [
    "# split dataframe into training, validation, and test dataframes\n",
    "df_train, df_val = train_test_split(df,test_size=0.2, random_state=42)\n",
    "df_train, df_test = train_test_split(df_train,test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Number of Training Images: \",len(df_train))\n",
    "print(\"Number of Validation Images: \", len(df_val))\n",
    "print(\"Number of Test Images: \", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPECIFY ARCHITECTURE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set some constant values for our model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "img_dir = \"../data/raw/\" # where images are stored\n",
    "img_width, img_height = 150,150 # resize images\n",
    "batch_size = 16 # to account for small training size, can increase to 32,64,etc\n",
    "no_epochs = 10 \n",
    "no_classes = 2 # choices of bird or not bird\n",
    "patience = 2 # used later for Early Stopping callback "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do random augmentation of our images which will help rescale them can help prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2240 validated image filenames belonging to 2 classes.\n",
      "CPU times: user 24 ms, sys: 8.78 ms, total: 32.8 ms\n",
      "Wall time: 31.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255, \n",
    "                    horizontal_flip = True,    \n",
    "                    zoom_range = 0.3,\n",
    "                    width_shift_range = 0.3,\n",
    "                    height_shift_range=0.3)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                    dataframe= df_train,\n",
    "                    directory=img_dir,\n",
    "                    x_col=\"file_name\",\n",
    "                    y_col=\"is_bird\",\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    class_mode=\"binary\",    \n",
    "                    target_size=(img_height,img_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment - Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 validated image filenames belonging to 2 classes.\n",
      "CPU times: user 11.9 ms, sys: 3.87 ms, total: 15.8 ms\n",
      "Wall time: 14.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "                    dataframe=df_val,\n",
    "                    directory=img_dir,\n",
    "                    x_col=\"file_name\",\n",
    "                    y_col=\"is_bird\",\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode=\"binary\",    \n",
    "                    target_size=(img_height,img_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with info from datacamp tutorial\n",
    "\n",
    "'''if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "'''\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100,activation = 'relu', input_shape = (img_width,img_height,3)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.13 ms, sys: 624 Âµs, total: 6.75 ms\n",
      "Wall time: 6.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compile, and add metrics==['accuracy'] to keep track of diagnostics for each epoch\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many epochs to go without model improving without stopping model\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=patience) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "140/140 - 132s - loss: 0.7326 - accuracy: 0.6719 - val_loss: 0.6848 - val_accuracy: 0.5763\n",
      "Epoch 2/7\n",
      "140/140 - 159s - loss: 0.6063 - accuracy: 0.6719 - val_loss: 0.5510 - val_accuracy: 0.7387\n",
      "Epoch 3/7\n",
      "140/140 - 157s - loss: 0.5963 - accuracy: 0.6817 - val_loss: 0.6601 - val_accuracy: 0.5775\n",
      "Epoch 4/7\n",
      "140/140 - 158s - loss: 0.5903 - accuracy: 0.6866 - val_loss: 0.5569 - val_accuracy: 0.7175\n",
      "CPU times: user 49min 33s, sys: 17min 27s, total: 1h 7min 1s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch= len(df_train) // batch_size,\n",
    "    epochs=no_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps= len(df_val) // batch_size,\n",
    "    callbacks=[early_stopping_monitor],\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 150, 150, 100)     400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150, 150, 100)     10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150, 150, 100)     10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 150, 150, 100)     10100     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2250000)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2250001   \n",
      "=================================================================\n",
      "Total params: 2,280,701\n",
      "Trainable params: 2,280,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (7,) and (4,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5d95297948b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (7,) and (4,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training and validation accuracy graphs, from kaggle\n",
    "# TO DO: find workaround to display graphs regardless of early stopping\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(no_epochs) # can break here; epochs stop at 7\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "file_name = 'original.h5'\n",
    "model.save('../models/{}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (IN PROGRESS):  PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: predictions\n",
    "\n",
    "'''model_1st = load_model('../models/{}'.format(file_name)) \n",
    "\n",
    "predictions = model_1st.predict(df_test)\n",
    "\n",
    "# in the example we had on datacamp, this was the second column - the probability a shot was made\n",
    "probability_type = predictions[:,1] \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO:  RESULTS & VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Lab 2.0\" -d -t -v -p numpy,pandas,tensorflow,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
